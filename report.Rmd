---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---
# Report Details

```{r}
articleID <- "27-6-2015" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- 'final'
pilotNames <- "Hannah Mieczkowski" # insert the pilot's name here e.g., "Tom Hardwicke". If there are multiple pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "Luiza Almeida Santos" # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- 180 # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- 30 # insert the co- pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- as.Date("11/01/18", format = "%m/%d/%y") # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- as.Date("05/09/18", format = "%m/%d/%y") # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- as.Date("11/01/18", format = "%m/%d/%y") # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

------

#### Methods summary: 
The authors asked 107 MBA students to express their gratitude to another person through letters as part of a voluntary class exercise. They were encouraged to write these to another MBA student in the program but this was not mandatory. Students then answered a questionnaire reporting their own experience and predicting the recipient's experience. They rated how surprised they thought the recipient would be after receiving the letter, how it would make the recipient feel and how awkward the recipient would feel. Students also reported on the current status of their relationship with the recipient.

------

#### Target outcomes: 

The target outcomes were: expectations vs. actual experiences of receiving the letters. This was measured through surprise, overall feelings, and awkwardness. 
T-tests were conducted on these measures.

From the article: *"Consistent with many findings reported in the existing literature (e.g., Lyubomirsky et al., 2011; Seligman et al., 2005), our results showed that writing a gratitude letter was a positive experience. Expressers reported being in a significantly more positive mood than normal (M = 2.58, SD = 1.30), one-sample t(106) = 20.59, p < .0001, d = 1.98, and reported being in a more positive mood after sending the gratitude letter than they did at the baseline measurement (M = 0.46, SD = 1.66), t(98) = 10.89, p < .0001, d = 1.09. 

As shown in Figure 1, expressers significantly underestimated how surprised recipients would be to receive the letter, paired-samples t(79) = 6.09, p < .0001, d = 0.68; underestimated how surprised they would be by the content of the letter, paired-samples t(79) = 3.49, p < .001, d =  .40; underestimated how positive recipients would feel, paired-samples t(79) = 6.60, p < .0001, d = 0.74; and overestimated how awkward recipients would feel, paired-samples t(79) = −2.89, p < .01, d = 0.32. Expressers believed that receiving gratitude would be a relatively positive experience, but it was even more positive for recipients than they expected. 

The correlations between expressers’ predicted ratings and their recipient’s actual ratings were consistently small, being significantly larger than zero only when expressers predicted surprise at receiving the letter (r = .35, p < .01). Accuracy correlations for predicted surprise at the content of the letter, mood, and awkwardness were all nonsignificantly different from zero (all ps > .2). Expressers did not appear to have great insight into their recipient’s unique experience, and they systematically underestimated how positive receiving gratitude would be for recipients."*

------
```{r global_options, include=FALSE}
# sets up some formatting options for the R Markdown document
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Step 1: Load packages and prepare report object
```{r}
# load packages
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports)# custom report functions
```


```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

# Step 2: Load data

```{r}
experiment1 = read_xls("/Users/Hannah/Downloads/CARPS_EXT_27-6-2015-master/data/Experiment1.xls")
```

# Step 3: Tidy data

```{r}
experiment1_tidy = experiment1 %>% filter(!is.na(RECIPIENT_GeneralSurpriseActual))

experiment1_tidy_plot = experiment1_tidy %>% gather(Type, Rating, SENDER_GeneralSurprise, RECIPIENT_GeneralSurpriseActual, SENDER_ContentSurprise, RECIPIENT_ContentSurpriseActual, SENDER_RecipientMoodPrediction, RECIPIENT_RecipientMoodActual,SENDER_RecipientAwkwardPrediction, RECIPIENT_RecipientAwkwardActual)

experiment1_tidy_plot_means = experiment1_tidy_plot %>% group_by(Type) %>% summarize(MeanRating = mean(Rating, na.rm = T))
```

# Step 4: Run analysis
## Inferential statistics

```{r}
t.test(experiment1$SENDER_SelfMoodActual, mu = 0, alternative = "two.sided")
t.test(experiment1$SENDER_BaselineMood, experiment1$SENDER_SelfMoodActual, paired = TRUE)
# Expressers reported being in a significantly more positive mood than normal (M = 2.58, SD = 1.30), one-sample t(106) = 20.59, p < .0001, d = 1.98, and reported being in a more positive mood after sending the gratitude letter than they did at the baseline measurement (M = 0.46, SD = 1.66), t(98) = 10.89, p < .0001, d = 1.09. 
##MATCH

t.test(experiment1_tidy$SENDER_GeneralSurprise, experiment1_tidy$RECIPIENT_GeneralSurpriseActual, paired = TRUE)
general = experiment1_tidy_plot_means[c(2,6),]
ggplot(general, aes(x=Type, y=MeanRating)) +
  geom_bar(position="dodge", stat="identity") 

t.test(experiment1_tidy$SENDER_ContentSurprise, experiment1_tidy$RECIPIENT_ContentSurpriseActual, paired = TRUE)
content = experiment1_tidy_plot_means[c(1,5),]
ggplot(content, aes(x=Type, y=MeanRating)) +
  geom_bar(position="dodge", stat="identity") 

t.test(experiment1_tidy$SENDER_RecipientAwkwardPrediction, experiment1_tidy$RECIPIENT_RecipientAwkwardActual, paired = TRUE)
awkward = experiment1_tidy_plot_means[c(3,7),]
ggplot(awkward, aes(x=Type, y=MeanRating)) +
  geom_bar(position="dodge", stat="identity")


t.test(experiment1_tidy$SENDER_RecipientMoodPrediction, experiment1_tidy$RECIPIENT_RecipientMoodActual, paired = TRUE)
mood = experiment1_tidy_plot_means[c(4,8),]
ggplot(mood, aes(x=Type, y=MeanRating)) +
  geom_bar(position="dodge", stat="identity")


# expressers significantly underestimated how surprised recipients would be to receive the letter, paired-samples t(79) = 6.09, p < .0001, d = 0.68; underestimated how surprised they would be by the content of the letter, paired-samples t(79) = 3.49, p < .001, d = 0.40; underestimated how positive recipients would feel, paired-samples t(79) = 6.60, p < .0001, d = 0.74; and overestimated how awkward recipients would feel, paired-samples t(79) = −2.89, p < .01, d = 0.32. Expressers believed that receiving gratitude would be a relatively positive experience, but it was even more positive for recipients than they expected. 
##MATCH

cor.test(experiment1_tidy$SENDER_GeneralSurprise, experiment1_tidy$RECIPIENT_GeneralSurpriseActual)
cor.test(experiment1_tidy$SENDER_ContentSurprise, experiment1_tidy$RECIPIENT_ContentSurpriseActual)
cor.test(experiment1_tidy$SENDER_RecipientAwkwardPrediction, experiment1_tidy$RECIPIENT_RecipientAwkwardActual)
cor.test(experiment1_tidy$SENDER_RecipientMoodPrediction, experiment1_tidy$RECIPIENT_RecipientMoodActual)
#The correlations between expressers’ predicted ratings and their recipient’s actual ratings were consistently small, being significantly larger than zero only when expressers predicted surprise at receiving the letter (r = .35, p < .01). Accuracy correlations for predicted surprise at the content of the letter, mood, and awkwardness were all nonsignificantly different from zero (all ps > .2).
##MATCH 
```

# Step 5: Conclusion

This reproducibility check supports the conclusions made in the original paper.

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- 0 # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- 0 # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- 0 # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- 0 # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- 0 # how many discrete issues were there for which you could not identify the cause

# How many of the above issues were resolved through author assistance?
locus_typo_resolved <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification_resolved <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis_resolved <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data_resolved <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified_resolved <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- FALSE # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? TRUE, FALSE, or NA. This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```


```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add variables to report 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome %in% c("MAJOR_ERROR", "DECISION_ERROR")) | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified, locus_typo_resolved, locus_specification_resolved, locus_analysis_resolved, locus_data_resolved, locus_unidentified_resolved)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
